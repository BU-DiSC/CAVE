# CAVE - Concurrency-Aware Graph Processing System for SSD

## Overview

*CAVE* is a graph processing engine for
storing, accessing, and performing graph analytics on SSDs. CAVE considers SSDâ€™s supported concurrency through its *internal parallelism* as a key property to exploit and it does so via issuing carefully tuned concurrent I/Os to the graphs stored on a single SSD. *CAVE* adopts a natural *blocked file* format based on adjacency
lists and uses a concurrent cache pool for data blocks to provide ease of implementation of different algorithms.

## Build & Compile

Goto `./cpp` folder, then run `make` for building.

```bash
cd cpp
make
```

**Clang** compiler is specified by default for its better support and performance of multi-threading. Using **GCC** is okay but we won't guarantee the performance.

We tested in the following configurations:

* RedHat Linux 4.9, Clang 11, EXT4 file system.
* Windows 11, GCC 13 + LLVM + UCRT toolchain from winlibs.com, NTFS file system.

## Usage

### Data File Parser

We developed a `parser` to convert common graph data into our binary file structure. It provides simple support for standard adjacent list and edge list files in plain text format for testing purpose. For more robust conversion, please check the `graph_parser.py` in `scripts` folder and documentation of [NetworkX package](https://networkx.org/).

Data files with suffix `.adjlist` and `.edgelist` will be automatically detected. Otherwise please indicate the file format by the `-format` argument.

```bash
./Parser <input_data_path> -format (adjlist/edgelist)
```

### Benchmark (main)

`main` is the entry point to run and time the algorithms with given parsed data file.

```bash
./main <parsed_data_path> (-arg_name arg_val)
```

The following are parameters for the benchmark.

* `-test_case`: Test case. 
  * `thread`: Run algorithms with different maximum **thread counts**. From 1 to 256 in power of 2.
  * `cache`: Vary the **cache pool size**. From 0 - 100%. Will test parallel BFS and WCC on the whole graph.
  * `fsize`: Test the effects of **maximum stack size** in parallel unordered DFS.
* `-test_algo`. Algorithms to run for `thread` test case.
  * `search`: Set random target keys and run BFS and DFS on the graph to find it.
  * `wcc`: Find the number of WCCs in the graph.
* `-nkeys`: Number of keys for `search` algorithms. The progrma will read keys in `./key/%data_name_%nkeys.txt`, or generate one randomly if not found.
* `-nrepeats`. Number of repeats for each test case.
* `-max_threads`. Maximum thread count. Override default value of `256` in `-test_case thread`.
* `-cache_size_mb`. Set cache size in megabytes for tests *other than* `cache` test. By default it's 10% of graph size.

Test results will be put in `output` folder in csv format.

## Example:

* Benchmark search algorithms varying thread counts on [CA-GrQc dataset](https://snap.stanford.edu/data/ca-GrQc.html), 10 keys, 5 repeats.

  ```bash
  # Parse data
  ./Parser ../data/CA-GrQc.txt -format edgelist

  # Benchmark
  ./main ../data/CA-GrQc.bin -test_case thread -test_algo search -nkeys 10 -nrepeats 5
  ```
* Cache size on [RoadNet-PA dataset](https://snap.stanford.edu/data/roadNet-PA.html), 3 repeats.

  ```bash
  # Parse data
  ./Parser ../data/RoadNet-PA.adjlist

  # Benchmark
  ./main ../data/RoadNet-PA.bin -test_case cache -nrepeats 3
  ```

## Algorithm

To demonstrate the benefits of our system, we implemented parallel algorithms of breadth-first search (BFS), depth-first search (DFS), and weakly connected components (WCC). These algorithms are in `GraphAlgorithm.cpp`, and can be executed and benchmarked by running `main` with the `-test_case` argument.

## Reference

Thread pool library comes from [BS::thread-pool]( https://github.com/bshoshany/thread-pool), a fast, lightweight, and easy-to-use C++17 thread pool library. 

Unordered parallel DFS is referred to ideas in [A work-efficient algorithm for parallel unordered depth-first search](https://dl.acm.org/doi/10.1145/2807591.2807651).
